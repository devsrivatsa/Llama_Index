{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "from llama_index import StorageContext, ServiceContext\n",
    "\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"research_paper_index\")\n",
    "index = load_index_from_storage(storage_context, index_id=\"graph_of_thoughts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import FunctionTool\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.agent import OpenAIAgent, ReActAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a simple tool\n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\"multiply 2 numbers and return the integer result\"\"\"\n",
    "    return a*b\n",
    "\n",
    "#it learns from the doc string.\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize llm\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In openai agent, tool selection is baked into it through fine tuning process - (selects tools given description)\n",
    "- In react agent, tool selection is computed with ReAct process. (open source llms, gpt4 - but requires more llm calls to select tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data agent = openai or react\n",
    "\n",
    "react_agent = ReActAgent.from_tools(tools=[multiply_tool],llm=llm, verbose=True)\n",
    "openai_agent = OpenAIAgent.from_tools(tools=[multiply_tool], llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query engines as tools\n",
    "\n",
    "[use this in conjunction with context retrieval and query planning and routing if needed. Or just plainly if the use case is simple.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagine if we have many different indices from which we can query\n",
    "uber_query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "lyft_query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=uber_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"uber_10k\",\n",
    "            description=\"Provides information about Uber financials for year 2021. Use a detailed plain text query as input to the tool.\"\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=lyft_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"lyft_10k\",\n",
    "            description=\"Provides information about Lyft financials for year 2021. Use a detailed plain text query as input to the tool.\"\n",
    "        )\n",
    "\n",
    "    )\n",
    "]\n",
    "\n",
    "# react_agent = ReActAgent.from_tools(\n",
    "#     tools=[multiply_tool, *query_engine_tools], \n",
    "#     llm=llm, \n",
    "#     verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use other Agents as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine_tools = [\n",
    "#     QueryEngineTool(\n",
    "#         query_engine=sql_agent,\n",
    "#         metadata=ToolMetadata(\n",
    "#             name=\"sql_agent\", description=\"Agent that can execute SQL queries.\"\n",
    "#         ),\n",
    "#     ),\n",
    "#     QueryEngineTool(\n",
    "#         query_engine=gmail_agent,\n",
    "#         metadata=ToolMetadata(\n",
    "#             name=\"gmail_agent\",\n",
    "#             description=\"Tool that can send emails on Gmail.\",\n",
    "#         ),\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# outer_agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving tools from an index; \n",
    "\n",
    "[use this for different function/sql/any tool and not for retrieval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n",
    "\n",
    "tool_mapping = SimpleToolNodeMapping.from_objects(query_engine_tools)\n",
    "object_index = ObjectIndex.from_objects(\n",
    "    query_engine_tools,\n",
    "    tool_mapping,\n",
    "    VectorStoreIndex\n",
    ")\n",
    "#this is deprecated\n",
    "# from llama_index.agent import FnRetrieverOpenAIAgent\n",
    "# openai_agent = FnRetrieverOpenAIAgent.from_retriever(\n",
    "#     object_index.as_retriever(),\n",
    "#     verbose=True\n",
    "# )\n",
    "#so use the following instead of FnRetrieverOpenAIAgent.from_re\n",
    "openai_agent = OpenAIAgent.from_tools(tool_retriever=object_index.as_retriever(), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Retrieval Agent\n",
    "\n",
    "Perform retrieval before calling tools --> pick better tools\n",
    "\n",
    "[can be used with action function tools or with query engine tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import ContextRetrieverOpenAIAgent\n",
    "\n",
    "# store a list of abbreviations\n",
    "texts = [\n",
    "    \"Abbreviation: X = Revenue\",\n",
    "    \"Abbreviation: YZ = Risk Factors\",\n",
    "    \"Abbreviation: Z = Costs\",\n",
    "]\n",
    "abbreviation_docs = [Document(text=t) for t in texts]\n",
    "context_index = VectorStoreIndex.from_documents(abbreviation_docs)\n",
    "\n",
    "# add context agent\n",
    "context_agent = ContextRetrieverOpenAIAgent.from_tools_and_retriever(\n",
    "    [march2023, feb2023, jan2023, dec2022],\n",
    "    context_index.as_retriever(similarity_top_k=1),\n",
    "    verbose=True\n",
    ")\n",
    "## response = context_agent.chat(\"what is YZ of march 2023 ?\") -> It will know where to look for and also understand what YZ means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Planning\n",
    "\n",
    "- infers a pydantic schema for complicated query plan\n",
    "- can be used in conjunction with context retrieval agent for more complicated situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryPlanTool\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "query_plan_tool = QueryPlanTool.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "openai_agent = OpenAIAgent.from_tools(\n",
    "    tools=[query_plan_tool],\n",
    "    max_function_calls=10,\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4-0613\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Alice' age=30 is_active=True\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class User(BaseModel):\n",
    "    \"\"\"\n",
    "    When you create an instance of User with user_data, Pydantic validates the data according to the schema, \n",
    "    ensuring that name is a string, age is an integer, etc. \n",
    "    If the data doesn't match the schema, it raises an error.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    age: int\n",
    "    is_active: bool = True\n",
    "\n",
    "# Example usage\n",
    "user_data = {'name': 'Alice', 'age': 30}\n",
    "user = User(**user_data)\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all topics in Model Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- utility tools - augment the capabilities of existing tools\n",
    "- check OnDemandToolLoader, and LoadAndSearchToolSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import BaseToolSpec\n",
    "# # class MyToolsSpec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External tools from llamahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.youtube_transcript import YoutubeTranscriptReader\n",
    "loader = YoutubeTranscriptReader()\n",
    "transcript_docs = loader.load_data(ytlinks=[\"https://www.youtube.com/watch?v=FNXIeEIu6LM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadhguru_index = VectorStoreIndex.from_documents(transcript_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadhguru_query_engine = sadhguru_index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sadhguru_query_engine.query(\"what is sadhguru trying to tell ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sadhguru is trying to convey the idea that instead of trying to release or '\n",
      " 'suppress emotions such as anger, lust, or hatred, one should learn to '\n",
      " 'transform and redirect that energy in the right direction. He emphasizes the '\n",
      " 'importance of not killing the energy but rather finding the right pathway '\n",
      " \"for it. Sadhguru also mentions that increasing one's energy levels through \"\n",
      " 'practices and methods can help in this process. Additionally, he suggests '\n",
      " \"that having a guru in one's life can provide guidance on where the energy \"\n",
      " 'should go. Ultimately, the goal is to accumulate enough energy to burst into '\n",
      " 'a different dimension or a higher state of being.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = sadhguru_index.as_retriever()\n",
    "nodes = retriever.retrieve(\"How to avoid anger ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Music]\n",
      "lust is great energy keep it inside cook\n",
      "[Music]\n",
      "yourself don't try to kill that energy\n",
      "redirect it in the right\n",
      "direction you have to burst into the\n",
      "next\n",
      "[Music]\n",
      "damage\n",
      "this is a big difference that you see in\n",
      "the understanding of Life between east\n",
      "and west east west has always talked\n",
      "about\n",
      "release everything you must release\n",
      "it your psychological situations you\n",
      "must find release either in somebody or\n",
      "something so the Easter way of life is\n",
      "always don't\n",
      "release whether it's your anger or your\n",
      "hatred or your lust whatever keep it\n",
      "inside cook\n",
      "yourself if you don't allow it any\n",
      "release it'll sh\n",
      "up this is\n",
      "your don't allow you to release anyway\n",
      "plug all the\n",
      "holes if you plug all the holes it will\n",
      "only go up so this is the way\n",
      "of this is where these philosophies are\n",
      "totally diametrical to other ways what\n",
      "they're saying is\n",
      "whatever you may do whether you get\n",
      "angry or you lustful or hatred the\n",
      "tremendous energy isn't it anger is\n",
      "great energy isn't\n",
      "it hatred is great energy lust is great\n",
      "energy all you have to learn is not to\n",
      "kill the energy not to release the\n",
      "energy just to transform the energy a\n",
      "little\n",
      "bit a man who is even incapable of anger\n",
      "definitely he'll be incapable of Love\n",
      "also definitely he'll be incapable of\n",
      "any great\n",
      "compassion If energy is bursting in the\n",
      "form of anger lust or hatred don't try\n",
      "to kill that energy all you have to do\n",
      "is transform it redirect it in the right\n",
      "direction because who you are is energy\n",
      "is it what you call as life is energy\n",
      "whatever that you are referring to as\n",
      "the Divine is also energy\n",
      "all you have to do is just find it the\n",
      "right kind of\n",
      "pathway you are accumulating of energy\n",
      "where it can be so threatening that he\n",
      "threatens your life he becomes so\n",
      "powerful within you that it threatens\n",
      "your very life unless energy is that\n",
      "powerful it will not get anywhere it\n",
      "will not carry you far\n",
      "enough and talking about this because so\n",
      "many of you have gone through these\n",
      "faces and when it gets too much you're\n",
      "looking for\n",
      "releases many times situations have come\n",
      "up within you that you can't bear it\n",
      "anymore then you go and find a release\n",
      "you want to read a book you want to\n",
      "switch on the television or you want to\n",
      "distract yourself you want to break\n",
      "break the practice for a few days for\n",
      "some reason you do this thing is because\n",
      "it's a release you're finding a release\n",
      "for yourself it feels\n",
      "good but if you go on releasing like\n",
      "this you will never have the necessary\n",
      "energy to burst into a different\n",
      "dimension you are not going to walk into\n",
      "the next Dimension you have to burst\n",
      "into the next Dimension so if you want\n",
      "to\n",
      "burst first thing you must block to a\n",
      "bursting\n",
      "point when you block sufficiently will\n",
      "break you\n",
      "so the practices that you're\n",
      "doing all the methods that we have used\n",
      "is\n",
      "mainly to increase the energy levels in\n",
      "such a way that gradually it finds its\n",
      "way if you allow it to find its way\n",
      "naturally it may take a long time and\n",
      "because you will not know where you're\n",
      "finding releases for it you will never\n",
      "know whether it's going somewhere or not\n",
      "going something so there is always a\n",
      "guidance to lead it on your business is\n",
      "just to raise your energy there it goes\n",
      "you don't worry once you have a guru in\n",
      "your life he takes care as to where it\n",
      "should go but it's your business to pump\n",
      "it\n",
      "up if you don't kick up enough energy\n",
      "then nothing can be done about\n",
      "[Music]\n",
      "it\n",
      "[Music]\n",
      "oh\n"
     ]
    }
   ],
   "source": [
    "for n in nodes:\n",
    "    print(n.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each type of index, there are specific retriever modes with specific purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Synthesizers\n",
    "\n",
    "With structured_answer_filtering set to True, our refine module is able to filter out any input nodes that are not relevant to the question being asked. This is particularly useful for RAG-based Q&A systems that involve retrieving chunks of text from external vector store for a given user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import Node\n",
    "from llama_index.response_synthesizers import ResponseMode, get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=ResponseMode.COMPACT,\n",
    "    structured_answer_filtering=True #response_mode needs to be compact or refine\n",
    ")\n",
    "# same as response_synthesizer = get_response_synthesizer(response_mode=\"compact\", structured_answer_filtering=True)\n",
    "\n",
    "# response = response_synthesizer.synthesize(\n",
    "#     \"query text\",\n",
    "#     nodes = nodes\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I cannot answer the query as there is no specific query text provided in the context information.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\n",
    "# query_engine.query(ask someting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response Modes:**\n",
    "\n",
    "    - refine\n",
    "    - compact\n",
    "    - tree_summarize\n",
    "    - simple_summarize\n",
    "    - no_text\n",
    "    - accumulate\n",
    "    - compact_accumulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom response synthesizers by inheriting from llama_index.response_synthesizers.base.BaseSynthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Prompt Templates (with additional variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import PromptTemplate\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"-----------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-----------------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge,\"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Please also write the answer in the tone of {tone_name}.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)\n",
    "\n",
    "summarizer = TreeSummarize(verbose=True, summary_template=qa_prompt)\n",
    "\n",
    "# response = summarizer.get_response(\"query\", [text], tone_name=\"a shakespeare play\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"\",\n",
    "    service_context=service_context,\n",
    "    text_qa_template=text_qa_tmpl,\n",
    "    refine_qa_template=refine_qa_tmpl,\n",
    "    use_async=False,\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "# response_synthesizer.synthesize() -> for synchronous\n",
    "# await response_synthesizer.asynthesize() -> for async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pydantic Tree Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.types import BaseModel\n",
    "\n",
    "\n",
    "class Biography(BaseModel):\n",
    "    \"\"\"Data model for a biography.\"\"\"\n",
    "    name: str\n",
    "    best_known_for: List[str]\n",
    "    extra_info: str\n",
    "\n",
    "summarizer = TreeSummarize(\n",
    "    verbose=True,\n",
    "    summary_template=qa_prompt,\n",
    "    output_cls=Biography\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = summarizer.get_response(\n",
    "#     \"who is Paul Graham?\", [text], tone_name=\"a business memo\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router\n",
    "\n",
    "It is basically something that chooses an option from multiple choices. The choosen option is the route it will take.\n",
    "\n",
    "It can be on top of a query engine or a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.retrievers.router_retriever import RouterRetriever\n",
    "from llama_index.selectors.pydantic_selectors import PydanticSingleSelector\n",
    "from llama_index.tools import QueryEngineTool, RetrieverTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not run\n",
    "\n",
    "list_query_engine = \"some query engine\"\n",
    "vector_query_engine = \"some other query engine\"\n",
    "\n",
    "list_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=list_query_engine,\n",
    "    description=\"Useful for summarizing questions related to data source.\"\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=\"Useful for retrieving specific context related to data source.\"\n",
    ")\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "        selector=PydanticSingleSelector.from_defaults(),\n",
    "        query_engine_tools=[\n",
    "            list_tool,\n",
    "            vector_tool\n",
    "        ]\n",
    ")\n",
    "\n",
    "# query_engine.query(\"something\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on top of a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not run\n",
    "\n",
    "vector_retriever = vector_tool.as_retriever()\n",
    "list_retriever = list_tool.as_retriever()\n",
    "\n",
    "vector_tool = RetrieverTool.from_defaults(\n",
    "    retriever=vector_retriever,\n",
    "    description=\"Useful for retrieving specific context from Quantum Physics\"\n",
    ")\n",
    "\n",
    "list_tool = RetrieverTool.from_defaults(\n",
    "    retriever=list_retriever,\n",
    "    description=\"Useful for retrieving specific context from CBC News\"\n",
    ")\n",
    "\n",
    "#routing\n",
    "retriever = RouterRetriever(\n",
    "    selector=PydanticSingleSelector.from_defaults(),\n",
    "    retriever_tools=[vector_tool, list_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selectors can be used separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.selectors.llm_selectors import (\n",
    "    LLMSingleSelector, #choose single from many choices\n",
    "    LLMMultiSelector, #choose multiple from many choices\n",
    ")\n",
    "\n",
    "from llama_index.selectors.pydantic_selectors import (\n",
    "    PydanticMultiSelector, #choose multiple from many choices\n",
    "    PydanticSingleSelector, #choose single from many choices\n",
    ")\n",
    "\n",
    "#individual llms can be finetuned in single tool and multi tool selection for a given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\n",
    "    ToolMetadata(description=\"description of tool1\", name=\"tool1\"),\n",
    "    ToolMetadata(description=\"description of tool2\", name=\"tool2\")\n",
    "]\n",
    "# [or]\n",
    "choices = [\n",
    "    \"tool 1 - description of tool1\",\n",
    "    \"tool 2 - description of tool2\"\n",
    "]\n",
    "\n",
    "selector = LLMSingleSelector.from_defaults()\n",
    "# result = selector.select(\n",
    "#     choices,\n",
    "#     query=\"select the good tool\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node PostProcessors\n",
    "\n",
    "- apply postprocessing after retrieving\n",
    "- happens inside query engine before response synthesis\n",
    "- accepts NodeWithScore objects only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor import (\n",
    "    SimilarityPostprocessor,\n",
    "    CohereRerank\n",
    ")\n",
    "from llama_index.schema import Node, NodeWithScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [\n",
    "    NodeWithScore(node=Node(text=\"something\"), score=0.7),\n",
    "    NodeWithScore(node=Node(text=\"some other thing\"), score=0.65),\n",
    "    NodeWithScore(node=Node(text=\"something other\"), score=0.45),\n",
    "    NodeWithScore(node=Node(text=\"something else\"), score=0.3)\n",
    "]\n",
    "\n",
    "#similarity node processor: only nodes that have a similarity score of 0.6 or more\n",
    "\n",
    "sim_processor = SimilarityPostprocessor(similarity_cutoff=0.6)\n",
    "filtered = sim_processor.postprocess_nodes(nodes)\n",
    "\n",
    "#rerank nodes given query string\n",
    "\n",
    "# reranker = CohereRerank(api_key=\"\", top_n=2)\n",
    "# filtered = reranker.postprocess_nodes(nodes)\n",
    "query_engine = index.as_query_engine(\n",
    "    node_postprocessors = [sim_processor] # + [reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here are summaries of some node postprocessor modules from LlamaIndex and examples of practical situations where they can be used:\n",
    "\n",
    "- SimilarityPostprocessor: This module removes nodes that are below a certain similarity score threshold. For instance, if you are gathering information from various sources and want to ensure that only the most relevant and closely related content is included, you can use this postprocessor to filter out less similar nodes​​.\n",
    "\n",
    "- KeywordNodePostprocessor: It ensures specific keywords are either included or excluded in the nodes. This is useful in situations where you want to focus on certain topics or avoid irrelevant ones. For example, if you are analyzing text for specific themes or concepts, you can set this postprocessor to include nodes containing specific keywords and exclude others​​.\n",
    "\n",
    "- MetadataReplacementPostProcessor: This postprocessor replaces the node content with a field from the node's metadata. It's most useful in combination with the SentenceWindowNodeParser. A practical use case could be when you're working with documents where certain metadata fields are more relevant than the main content, allowing you to swap the node content with this key information​​.\n",
    "\n",
    "- LongContextReorder: This module re-orders retrieved nodes, which is beneficial when dealing with large amounts of data and where crucial data might be buried in the middle of long contexts. It helps in scenarios where you need to sift through extensive data and want the most relevant information to be more accessible.\n",
    "\n",
    "- others are rerankers and simple to understand\n",
    "\n",
    "\n",
    "We can also have custom node post processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "# from guidance.llms import OpenAI\n",
    "from llama_index.program import GuidancePydanticProgram, OpenAIPydanticProgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Output Schema\n",
    "\n",
    "class Song(BaseModel):\n",
    "    title: str\n",
    "    length_sec: int\n",
    "\n",
    "class Album(BaseModel):\n",
    "    name: str\n",
    "    artist: str\n",
    "    songs: List[Song]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI Pydantic program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tmpl_str = (\n",
    "    \"Generate an example album, with artist and a list of songs. Using the movie {movie_name} as inspiration\"\n",
    ")\n",
    "\n",
    "program = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=Album,\n",
    "    prompt_template_str=prompt_tmpl_str,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "#for parallel function calling:\n",
    "program_parallel = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=Album,\n",
    "    prompt_template_str=prompt_tmpl_str,\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo-1106\"),\n",
    "    allow_multiple=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- can also do streaming.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guidance pydantic program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tmpl_str = (\n",
    "    \"Generate an example album, with artist and a list of songs. Using the movie {{movie_name}} as inspiration\"\n",
    ")\n",
    "\n",
    "program = GuidancePydanticProgram(\n",
    "    output_cls=Album,\n",
    "    prompt_template_str=prompt_tmpl_str,\n",
    "    guidance_llm=OpenAI(\"text-davinci-003\"),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# output = program(movie_name=\"Outlander\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
