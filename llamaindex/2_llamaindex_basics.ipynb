{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "from llama_index import StorageContext, ServiceContext\n",
    "\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"research_paper_index\")\n",
    "index = load_index_from_storage(storage_context, index_id=\"graph_of_thoughts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import FunctionTool\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.agent import OpenAIAgent, ReActAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a simple tool\n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\"multiply 2 numbers and return the integer result\"\"\"\n",
    "    return a*b\n",
    "\n",
    "#it learns from the doc string.\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize llm\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In openai agent, tool selection is baked into it through fine tuning process - (selects tools given description)\n",
    "- In react agent, tool selection is computed with ReAct process. (open source llms, gpt4 - but requires more llm calls to select tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data agent = openai or react\n",
    "\n",
    "react_agent = ReActAgent.from_tools(tools=[multiply_tool],llm=llm, verbose=True)\n",
    "openai_agent = OpenAIAgent.from_tools(tools=[multiply_tool], llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query engines as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imagine if we have many different indices from which we can query\n",
    "uber_query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "lyft_query_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=uber_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"uber_10k\",\n",
    "            description=\"Provides information about Uber financials for year 2021. Use a detailed plain text query as input to the tool.\"\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=lyft_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"lyft_10k\",\n",
    "            description=\"Provides information about Lyft financials for year 2021. Use a detailed plain text query as input to the tool.\"\n",
    "        )\n",
    "\n",
    "    )\n",
    "]\n",
    "\n",
    "react_agent = ReActAgent.from_tools(\n",
    "    tools=[multiply_tool, *query_engine_tools], \n",
    "    llm=llm, \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use other Agents as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine_tools = [\n",
    "#     QueryEngineTool(\n",
    "#         query_engine=sql_agent,\n",
    "#         metadata=ToolMetadata(\n",
    "#             name=\"sql_agent\", description=\"Agent that can execute SQL queries.\"\n",
    "#         ),\n",
    "#     ),\n",
    "#     QueryEngineTool(\n",
    "#         query_engine=gmail_agent,\n",
    "#         metadata=ToolMetadata(\n",
    "#             name=\"gmail_agent\",\n",
    "#             description=\"Tool that can send emails on Gmail.\",\n",
    "#         ),\n",
    "#     ),\n",
    "# ]\n",
    "\n",
    "# outer_agent = ReActAgent.from_tools(query_engine_tools, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving tools from an index; Query Planning: only oai agents, not react for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.objects import ObjectIndex, SimpleToolNodeMapping\n",
    "from llama_index.agent import FnRetrieverOpenAIAgent\n",
    "\n",
    "all_tools = query_engine_tools + [multiply_tool]\n",
    "tool_mapping = SimpleToolNodeMapping.from_objects(all_tools)\n",
    "object_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    tool_mapping,\n",
    "    VectorStoreIndex\n",
    ")\n",
    "openai_agent = FnRetrieverOpenAIAgent.from_retriever(\n",
    "    object_index.as_retriever(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Retrieval Agent\n",
    "\n",
    "Perform retrieval before calling tools --> pick better tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import ContextRetrieverOpenAIAgent\n",
    "\n",
    "# store a list of abbreviations\n",
    "texts = [\n",
    "    \"Abbreviation: X = Revenue\",\n",
    "    \"Abbreviation: YZ = Risk Factors\",\n",
    "    \"Abbreviation: Z = Costs\",\n",
    "]\n",
    "abbreviation_docs = [Document(text=t) for t in texts]\n",
    "context_index = VectorStoreIndex.from_documents(abbreviation_docs)\n",
    "\n",
    "# add context agent\n",
    "context_agent = ContextRetrieverOpenAIAgent.from_tools_and_retriever(\n",
    "    all_tools,\n",
    "    context_index.as_retriever(similarity_top_k=1),\n",
    "    verbose=True\n",
    ")\n",
    "## response = context_agent.chat(\"what is YZ of march 2022 ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Planning\n",
    "\n",
    "- infers a pydantic schema for complicated query plan\n",
    "- can be used in conjunction with context retrieal agent for more complicated situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryPlanTool\n",
    "\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "query_plan_tool = QueryPlanTool.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")\n",
    "\n",
    "openai_agent = OpenAIAgent.from_tools(\n",
    "    tools=[query_plan_tool],\n",
    "    max_function_calls=10,\n",
    "    llm=OpenAI(temperature=0, model=\"gpt-4-0613\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Alice' age=30 is_active=True\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class User(BaseModel):\n",
    "    \"\"\"\n",
    "    When you create an instance of User with user_data, Pydantic validates the data according to the schema, \n",
    "    ensuring that name is a string, age is an integer, etc. \n",
    "    If the data doesn't match the schema, it raises an error.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    age: int\n",
    "    is_active: bool = True\n",
    "\n",
    "# Example usage\n",
    "user_data = {'name': 'Alice', 'age': 30}\n",
    "user = User(**user_data)\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all topics in Model Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- utility tools - augment the capabilities of existing tools\n",
    "- check OnDemandToolLoader, and LoadAndSearchToolSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tool Specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import BaseToolSpec\n",
    "# # class MyToolsSpec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### External tools from llamahub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_hub import YouubeTranscriptReader\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_hub.youtube_transcript import YoutubeTranscriptReader\n",
    "loader = YoutubeTranscriptReader()\n",
    "transcript_docs = loader.load_data(ytlinks=[\"https://www.youtube.com/watch?v=FNXIeEIu6LM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadhguru_index = VectorStoreIndex.from_documents(transcript_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sadhguru_query_engine = sadhguru_index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sadhguru_query_engine.query(\"what is sadhguru trying to tell ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sadhguru is trying to convey the idea that instead of trying to release or '\n",
      " 'suppress emotions such as anger, lust, or hatred, one should learn to '\n",
      " 'transform and redirect that energy in the right direction. He emphasizes the '\n",
      " 'importance of not killing the energy but rather finding the right pathway '\n",
      " \"for it. Sadhguru also mentions that increasing one's energy levels through \"\n",
      " 'practices and methods can help in this process. Additionally, he suggests '\n",
      " \"that having a guru in one's life can provide guidance on where the energy \"\n",
      " 'should go. Ultimately, the goal is to accumulate enough energy to burst into '\n",
      " 'a different dimension or a higher state of being.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = sadhguru_index.as_retriever()\n",
    "nodes = retriever.retrieve(\"How to avoid anger ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Music]\n",
      "lust is great energy keep it inside cook\n",
      "[Music]\n",
      "yourself don't try to kill that energy\n",
      "redirect it in the right\n",
      "direction you have to burst into the\n",
      "next\n",
      "[Music]\n",
      "damage\n",
      "this is a big difference that you see in\n",
      "the understanding of Life between east\n",
      "and west east west has always talked\n",
      "about\n",
      "release everything you must release\n",
      "it your psychological situations you\n",
      "must find release either in somebody or\n",
      "something so the Easter way of life is\n",
      "always don't\n",
      "release whether it's your anger or your\n",
      "hatred or your lust whatever keep it\n",
      "inside cook\n",
      "yourself if you don't allow it any\n",
      "release it'll sh\n",
      "up this is\n",
      "your don't allow you to release anyway\n",
      "plug all the\n",
      "holes if you plug all the holes it will\n",
      "only go up so this is the way\n",
      "of this is where these philosophies are\n",
      "totally diametrical to other ways what\n",
      "they're saying is\n",
      "whatever you may do whether you get\n",
      "angry or you lustful or hatred the\n",
      "tremendous energy isn't it anger is\n",
      "great energy isn't\n",
      "it hatred is great energy lust is great\n",
      "energy all you have to learn is not to\n",
      "kill the energy not to release the\n",
      "energy just to transform the energy a\n",
      "little\n",
      "bit a man who is even incapable of anger\n",
      "definitely he'll be incapable of Love\n",
      "also definitely he'll be incapable of\n",
      "any great\n",
      "compassion If energy is bursting in the\n",
      "form of anger lust or hatred don't try\n",
      "to kill that energy all you have to do\n",
      "is transform it redirect it in the right\n",
      "direction because who you are is energy\n",
      "is it what you call as life is energy\n",
      "whatever that you are referring to as\n",
      "the Divine is also energy\n",
      "all you have to do is just find it the\n",
      "right kind of\n",
      "pathway you are accumulating of energy\n",
      "where it can be so threatening that he\n",
      "threatens your life he becomes so\n",
      "powerful within you that it threatens\n",
      "your very life unless energy is that\n",
      "powerful it will not get anywhere it\n",
      "will not carry you far\n",
      "enough and talking about this because so\n",
      "many of you have gone through these\n",
      "faces and when it gets too much you're\n",
      "looking for\n",
      "releases many times situations have come\n",
      "up within you that you can't bear it\n",
      "anymore then you go and find a release\n",
      "you want to read a book you want to\n",
      "switch on the television or you want to\n",
      "distract yourself you want to break\n",
      "break the practice for a few days for\n",
      "some reason you do this thing is because\n",
      "it's a release you're finding a release\n",
      "for yourself it feels\n",
      "good but if you go on releasing like\n",
      "this you will never have the necessary\n",
      "energy to burst into a different\n",
      "dimension you are not going to walk into\n",
      "the next Dimension you have to burst\n",
      "into the next Dimension so if you want\n",
      "to\n",
      "burst first thing you must block to a\n",
      "bursting\n",
      "point when you block sufficiently will\n",
      "break you\n",
      "so the practices that you're\n",
      "doing all the methods that we have used\n",
      "is\n",
      "mainly to increase the energy levels in\n",
      "such a way that gradually it finds its\n",
      "way if you allow it to find its way\n",
      "naturally it may take a long time and\n",
      "because you will not know where you're\n",
      "finding releases for it you will never\n",
      "know whether it's going somewhere or not\n",
      "going something so there is always a\n",
      "guidance to lead it on your business is\n",
      "just to raise your energy there it goes\n",
      "you don't worry once you have a guru in\n",
      "your life he takes care as to where it\n",
      "should go but it's your business to pump\n",
      "it\n",
      "up if you don't kick up enough energy\n",
      "then nothing can be done about\n",
      "[Music]\n",
      "it\n",
      "[Music]\n",
      "oh\n"
     ]
    }
   ],
   "source": [
    "for n in nodes:\n",
    "    print(n.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each type of index, there are specific retriever modes with specific purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Synthesizers\n",
    "\n",
    "With structured_answer_filtering set to True, our refine module is able to filter out any input nodes that are not relevant to the question being asked. This is particularly useful for RAG-based Q&A systems that involve retrieving chunks of text from external vector store for a given user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import Node\n",
    "from llama_index.response_synthesizers import ResponseMode, get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=ResponseMode.COMPACT,\n",
    "    structured_answer_filtering=True #response_mode needs to be compact or refine\n",
    ")\n",
    "# same as response_synthesizer = get_response_synthesizer(response_mode=\"compact\", structured_answer_filtering=True)\n",
    "\n",
    "# response = response_synthesizer.synthesize(\n",
    "#     \"query text\",\n",
    "#     nodes = nodes\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I cannot answer the query as there is no specific query text provided in the context information.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(response_synthesizer=response_synthesizer)\n",
    "# query_engine.query(ask someting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response Modes:**\n",
    "\n",
    "    - refine\n",
    "    - compact\n",
    "    - tree_summarize\n",
    "    - simple_summarize\n",
    "    - no_text\n",
    "    - accumulate\n",
    "    - compact_accumulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom response synthesizers by inheriting from llama_index.response_synthesizers.base.BaseSynthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Prompt Templates (with additional variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import PromptTemplate\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "\n",
    "qa_prompt_tmpl = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"-----------------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"-----------------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge,\"\n",
    "    \"answer the query.\\n\"\n",
    "    \"Please also write the answer in the tone of {tone_name}.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "qa_prompt = PromptTemplate(qa_prompt_tmpl)\n",
    "\n",
    "summarizer = TreeSummarize(verbose=True, summary_template=qa_prompt)\n",
    "\n",
    "# response = summarizer.get_response(\"query\", [text], tone_name=\"a shakespeare play\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"\",\n",
    "    service_context=service_context,\n",
    "    text_qa_template=text_qa_tmpl,\n",
    "    refine_qa_template=refine_qa_tmpl,\n",
    "    use_async=False,\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "# response_synthesizer.synthesize() -> for synchronous\n",
    "# await response_synthesizer.asynthesize() -> for async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pydantic Tree Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.types import BaseModel\n",
    "\n",
    "\n",
    "class Biography(BaseModel):\n",
    "    \"\"\"Data model for a biography.\"\"\"\n",
    "    name: str\n",
    "    best_known_for: List[str]\n",
    "    extra_info: str\n",
    "\n",
    "summarizer = TreeSummarize(\n",
    "    verbose=True,\n",
    "    summary_template=qa_prompt,\n",
    "    output_cls=Biography\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = summarizer.get_response(\n",
    "#     \"who is Paul Graham?\", [text], tone_name=\"a business memo\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router\n",
    "\n",
    "It is basically something that chooses an option from multiple choices. The choosen option is the route it will take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do not run\n",
    "\n",
    "from llama_index.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.selectors.pydantic_selectors import PydanticSingleSelector\n",
    "from llama_index.tools.query_engine import QueryEngineTool\n",
    "\n",
    "list_query_engine = \"some query engine\"\n",
    "vector_query_engine = \"some other query engine\"\n",
    "\n",
    "list_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=list_query_engine,\n",
    "    description=\"Useful for summarizing questions related to data source.\"\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=\"Useful for retrieving specific context related to data source.\"\n",
    ")\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "        selector=PydanticSingleSelector.from_defaults(),\n",
    "        query_engine_tools=[\n",
    "            list_tool,\n",
    "            vector_tool\n",
    "        ]\n",
    ")\n",
    "\n",
    "# query_engine.query(\"something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
