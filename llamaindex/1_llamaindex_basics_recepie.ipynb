{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from Ipython.display import Markdown, display\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    get_response_synthesizer,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage\n",
    ")\n",
    "\n",
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./src_docs/current_subset\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing and Storing:\n",
    "\n",
    "Once stored, there is no need to index them again as this will result in openai api calls to converting documents into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high level api\n",
    "# service_context = ServiceContext.from_defaults(chunk_size=512, chunk_overlap=10)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     service_context=service_context\n",
    "# )\n",
    "\n",
    "\n",
    "# [ or ]\n",
    "\n",
    "#low level api: customize the parser\n",
    "# parser = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "# service_context = ServiceContext.from_defaults(text_splitter=parser)\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#               documents, \n",
    "#               service_context=service_context, \n",
    "#               show_progress=True\n",
    "#)\n",
    "\n",
    "# index.set_index_id(\"research_papers_vector_index\")\n",
    "# index.storage_context.persist(\"./storage\")\n",
    "\n",
    "\n",
    "# [ or ]\n",
    "\n",
    "#further low level api: customize the parser\n",
    "# parser = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "# nodes = parser.get_nodes_from_documents(documents)\n",
    "# index = VectorStoreIdex(nodes)\n",
    "# service_context = ServiceContext.from_defaults(text_splitter=parser)\n",
    "# index.set_index_id(\"research_papers_vector_index\")\n",
    "# index.storage_context.persist(\"./storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload index from local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
    "index = load_index_from_storage(\n",
    "    storage_context, \n",
    "    index_id=\"research_papers_vector_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure a Retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure response synthesis\n",
    "response_synthesizer = get_response_synthesizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"Can you give me an overview of graph of thoughts in an easy to understand manner?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Graph of Thoughts (GoT) is a framework that enhances the capabilities of large language models (LLMs) by representing the information generated by an LLM as a graph. In this graph, each unit of information, known as an \"LLM thought,\" is represented as a vertex, and the relationships between these thoughts are represented as edges.\n",
      "\n",
      "GoT allows for the combination of different LLM thoughts, enabling the LLM to solve complex problems by leveraging the connections between thoughts. It supports various thought transformations, including aggregation, refining, and generation.\n",
      "\n",
      "Aggregation involves combining multiple thoughts into a new thought, allowing for the synthesis of whole networks of thoughts. Refining allows for the modification of a current thought, while generation involves creating new thoughts based on existing ones.\n",
      "\n",
      "Thoughts in GoT can be scored and ranked to evaluate their quality and relevance. This helps in assessing the effectiveness of the current solution and selecting the most relevant thoughts for further processing.\n",
      "\n",
      "Overall, GoT provides a flexible and powerful framework for leveraging the relationships between thoughts and enhancing the capabilities of LLMs.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_query_engine = index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tot_query_engine.query(\n",
    "    \"What are the key concepts involved in understanding graph of thoughts. Can you summarize and explain each of them shortly ?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key concepts involved in understanding the Graph of Thoughts (GoT) are as follows:\n",
      "\n",
      "1. Graph of Thoughts (GoT): GoT is an approach that enhances the capabilities of Language Model Machines (LLMs) through networked reasoning. It models LLM thoughts as vertices and dependencies between thoughts as edges in a directed graph. GoT allows for the aggregation of arbitrary thoughts by constructing vertices with multiple incoming edges, enabling more complex thought patterns.\n",
      "\n",
      "2. Thought Transformations: GoT enables various thought transformations, such as aggregating thoughts into a new one, refining thoughts through looping, and backtracking from a chain of thoughts. These transformations enhance the flexibility and power of LLMs in generating solutions.\n",
      "\n",
      "3. Evaluator Function (E) and Ranking Function (R): In the GoT framework, an evaluator function (E) is used to assign scores to thoughts, evaluating their relevance or quality. A ranking function (R) is then employed to select the most relevant thoughts based on their scores. These functions play a crucial role in guiding the LLM's reasoning process.\n",
      "\n",
      "4. Directed Graph (G): The reasoning process in GoT is modeled as a directed graph (G) consisting of vertices (thoughts) and edges (dependencies between thoughts). The directed edges indicate that a thought has been constructed using another thought as direct input. The graph structure allows for the representation of complex relationships and dependencies between thoughts.\n",
      "\n",
      "5. Classes of Graph Nodes: In certain use cases, the nodes (thoughts) in the GoT graph can belong to different classes. The specific form of a thought depends on the use case, such as a paragraph in writing tasks or a sequence of numbers in sorting. This flexibility accommodates diverse applications of the GoT framework.\n",
      "\n",
      "Overall, the Graph of Thoughts (GoT) framework provides a way to model and manipulate LLM thoughts using a graph structure, enabling more advanced reasoning and thought transformations.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- maximum marginal relevance\n",
    "- custom embedding string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: f7b7f7b3-2285-4c36-aa96-5635a53d194f): For example, one could explore a certain chain of reason-\\ning, backtrack and start a new one, the...\\n\\n> Source (Doc id: b42f9f15-951b-4e28-b1aa-d367287e811b): Input\\nOutputInput\\nOutput OutputThoughts:\\nUnscored\\nNegative\\nscore OutputInput\\nOutput[This work]\\nIn...'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### query with filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.vector_stores.types import ExactMatchFiler, MetadataFilters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        ExactMatchFiler(key=\"tag\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=20,\n",
    "    filters=filters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Metadata Extraction\n",
    "\n",
    "Disambiguate similar looking passages by extracting important keywords from the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors import (\n",
    "    SummaryExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    "    KeywordExtractor,\n",
    "    EntityExtractor\n",
    ")\n",
    "from llama_index.ingestion import IngestionPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srivatsa/my_jupyter_noteboks/llamaindex/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 5.08k/5.08k [00:00<00:00, 897kB/s]\n",
      "model.safetensors: 100%|██████████| 712M/712M [00:11<00:00, 63.4MB/s] \n",
      "config.json: 100%|██████████| 625/625 [00:00<00:00, 89.0kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 5.90kB/s]\n",
      "vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 20.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.96M/1.96M [00:00<00:00, 16.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "transformations = [\n",
    "    SentenceSplitter(),\n",
    "    TitleExtractor(nodes=5),\n",
    "    QuestionsAnsweredExtractor(questions=3),\n",
    "    SummaryExtractor(summaries=[\"prev\", \"self\"]),\n",
    "    KeywordExtractor(keywords=10),\n",
    "    EntityExtractor(prediction_threshold=0.5)\n",
    "]\n",
    "pileline = IngestionPipeline(transformations=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will make many llm calls\n",
    "\n",
    "# nodes = pileline.run(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.extractors import BaseExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a more advanced example, it can also make use of an llm to extract features from the node content and the existing metadata. Refer to the source code of the provided metadata extractors for more details.\n",
    "\n",
    "class CustomExtractor(BaseExtractor):\n",
    "    def extract(self, nodes):\n",
    "        metadata_list = [\n",
    "            {\n",
    "                \"custom\":node.metadata[\"document_title\"]\n",
    "                + \"\\n\"\n",
    "                + node.metadata[\"excerpt_keywords\"]\n",
    "            } for node in nodes\n",
    "        ]\n",
    "        return metadata_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex, Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = SummaryIndex([])\n",
    "text_chunks = [\"The cat's name is Cash\", \"I love quantum mechnics\", \"I want to buy a tesla\"]\n",
    "doc_chunks = []\n",
    "for i, text in enumerate(text_chunks):\n",
    "    doc = Document(text=text, id=f\"doc_id_{i}\")\n",
    "    doc_chunks.append(doc)\n",
    "#insert\n",
    "for doc_chunk in doc_chunks:\n",
    "    index.insert(doc_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete\n",
    "index.delete_ref_doc(\"The cat's name is Cash\", delete_from_docstore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The cat's name is Cash\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update\n",
    "\n",
    "# Here, we passed some extra kwargs to ensure the document is deleted from the docstore. This is of course optional.\n",
    "doc_chunks[0].text = \"Who is Fred Flinstones?\"\n",
    "index.update_ref_doc(\n",
    "    doc_chunks[0],\n",
    "    update_kwargs={\"delete_kwargs\":{\"delete_from_docstore\":True}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False, False, True]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#refresh: (use case: constantly updating new information such as a directory, the working context memory, etc)\n",
    "\n",
    "# modify first document, with the same doc_id\n",
    "doc_chunks[0] = Document(text=\"Andrew Huberman podcast is amazing!\", id_=\"doc_id_0\")\n",
    "\n",
    "#add a new document\n",
    "doc_chunks.append(\n",
    "    Document(\n",
    "        text=\"this isn't in the index yet, but it will soon be\",\n",
    "        id_=\"doc_id_2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "refreshed_docs = index.refresh_ref_docs(\n",
    "    doc_chunks, \n",
    "    update_kwargs={\"delete_kwargs\": {\"delete_from_docstpre\": True}}\n",
    ")\n",
    "refreshed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cd602e96-f474-415e-8b6f-f5a4e0de2ee2': RefDocInfo(node_ids=['4b76d51f-02e5-4a20-8321-8707781392dd'], metadata={}),\n",
       " 'cdd3620c-63f3-415a-b478-96cb47cd2404': RefDocInfo(node_ids=['3cbc1dd9-4310-4b39-bcc1-cff65261a6e2'], metadata={}),\n",
       " '9f28c6bb-dbf6-4d7c-8360-f2ee43442af2': RefDocInfo(node_ids=['0934edfa-4f20-4351-a174-ea47f6ba5f71'], metadata={}),\n",
       " 'doc_id_0': RefDocInfo(node_ids=['f2b83001-4551-4b3b-8263-e357a99a4bb7'], metadata={}),\n",
       " 'doc_id_2': RefDocInfo(node_ids=['2a42784c-5f21-4d95-ad64-486fa328085a'], metadata={})}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ref_doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Customizing Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'9ca2a1d0-3497-481b-bce2-e039f68cffd2': RefDocInfo(node_ids=['488f81f3-4839-4375-94cb-83bf87e180ad', '76cbc7da-ebd5-4562-84a4-921c8e3aa2b4'], metadata={'page_label': '1', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " 'ef5bc24e-11ab-42d6-b1c5-4db1ff1239cc': RefDocInfo(node_ids=['7a7849cc-1a77-4ec9-9c80-c44ebe80ed77', '17f91e8d-5fcb-407b-95f9-c4146192f5ec'], metadata={'page_label': '2', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " '79786b26-060e-4428-85c2-d57843c47428': RefDocInfo(node_ids=['e171e276-5a7c-45d7-adb0-20d7dbb0c9c9', '03ee9972-173b-491b-80f6-5341d812c179'], metadata={'page_label': '3', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " 'db78872b-b53c-4868-a04e-1d03e3198e42': RefDocInfo(node_ids=['f0e63625-b542-4772-b2e6-35a25642bad0', 'c538b903-dcea-4c73-b50e-e20e0b478502'], metadata={'page_label': '4', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " '44f84dcf-f7b0-4ac4-a1d4-0daeb5c8ad03': RefDocInfo(node_ids=['77584a05-924b-4120-9b18-b028f3ca0695', '4ebfa66d-5fdf-4715-8387-590ede260290', '92f47e1e-cbe6-49ab-86d3-30943a33429e', 'c8898f53-c221-4e02-8800-11e8c0d76efd'], metadata={'page_label': '5', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " 'e13766eb-e5e8-46ad-acb0-e3c44400bc7d': RefDocInfo(node_ids=['695981b4-720c-4539-84b8-985053420acd', '653d7322-e8d6-40ed-ae7c-f2029cd5a3ed'], metadata={'page_label': '6', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " '0321b819-7f1f-49ea-9981-2a22e01d46cf': RefDocInfo(node_ids=['04307cc4-b67e-4a21-ba80-30c94a4447fe', 'b695512d-b244-4dba-b776-3e4b817d425a'], metadata={'page_label': '7', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " '3c0b9233-573d-4df6-b237-1c4f8ef62827': RefDocInfo(node_ids=['54b6a852-b759-4601-9d60-ed2df667073b', 'd5162212-dc71-4446-a674-344f8213e10e'], metadata={'page_label': '8', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " 'a8cb6ac5-155d-46b8-aeac-6a4db224ec00': RefDocInfo(node_ids=['a14568e4-6adc-4d8f-8b89-d262d7e5d5ee'], metadata={'page_label': '9', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " '07ba6b56-0996-462c-9f05-399dffad92ac': RefDocInfo(node_ids=['4c5d2462-46c6-4627-93ef-8d73614e1d9a', '96c652c5-2d2c-4091-a10d-74c41c297546'], metadata={'page_label': '10', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " '66c5e0d0-348a-40e6-867e-d1746d9673ce': RefDocInfo(node_ids=['6a7e28c1-8457-4cbd-9782-4b132156ca70', '10d6add5-ad60-4b84-845a-b7337ea9bba0', 'd7edbe94-649f-410b-a9df-0db6ff11af19'], metadata={'page_label': '11', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " 'ddd18cc5-dfdc-414a-92d7-6511e3cfee28': RefDocInfo(node_ids=['5f45883a-ad2f-42c0-a947-60103cde33ee', 'c910faf2-1972-4694-81a4-845ffdb6e68e', '64c82def-3cfd-42ee-a23c-29474f67af84'], metadata={'page_label': '12', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'}),\n",
       " '41509996-f13e-436a-aaaa-166140364176': RefDocInfo(node_ids=['85b6fe73-c37b-4cac-9106-cb98a0a2a84b', '002662aa-4284-4077-a483-a3abd16f7213'], metadata={'page_label': '13', 'file_name': 'graph_of_thoughts.pdf', 'file_path': 'src_docs/current_subset/graph_of_thoughts.pdf', 'file_type': 'application/pdf', 'file_size': 2838941, 'creation_date': '2023-11-25', 'last_modified_date': '2023-11-25', 'last_accessed_date': '2023-11-25'})}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ref_doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "from llama_index.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a parser and parse the documents into nodes\n",
    "parser = SentenceSplitter()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "#create a storage context using default stores\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore(),\n",
    "    vector_store=SimpleVectorStore(),\n",
    "    index_store=SimpleIndexStore()\n",
    ")\n",
    "\n",
    "#create or load docstore and add nodes\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "#build index -comment out after creating index\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "#set index_id and save index -comment out after creating index\n",
    "index.set_index_id(\"graph_of_thoughts\")\n",
    "index.storage_context.persist(persist_dir=\"./research_paper_index\")\n",
    "\n",
    "#load index from memory\n",
    "from llama_index import load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./research_paper_index\")\n",
    "index = load_index_from_storage(storage_context)\n",
    "\n",
    "# # if loading an index from a persist_dir containing multiple indexes\n",
    "# loaded_index = load_index_from_storage(storage_context, index_id=\"<index_id>\")\n",
    "\n",
    "# # if loading multiple indexes from a persist dir\n",
    "# loaded_indicies = load_index_from_storage(\n",
    "#     storage_context, index_ids=[\"<index_id>\", ...]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    load_index_from_storage,\n",
    "    load_indices_from_storage,\n",
    "    load_graph_from_storage\n",
    ")\n",
    "from llama_index import load_index_from_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load index \n",
    "#[changed default__vector_store.json to vector_store.json in ./research_paper_index]\n",
    "\n",
    "persist_dir = \"./research_paper_index\"\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(persist_dir),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir),\n",
    ")\n",
    "\n",
    "index = load_index_from_storage(storage_context, index_id=\"graph_of_thoughts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can also load and save index from cloud/remote storaage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vector stores contain embedding vectors of ingested document chunks (and sometimes the document chunks themselves)\n",
    "    - chromadb, pinecone, wewiate\n",
    "- document store contain ingested document chunks (i.e node objects - with embedding, metadata, other data etc.)\n",
    "    - mongodb, redis\n",
    "- index stores contain lightweight index metadata (additional state information about index)\n",
    "    - mongodb, redis\n",
    "- key-value stores are just key value stores. (not ready yet)\n",
    "- graph stores store graph data\n",
    "    - neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "#query_engine.query(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high level api\n",
    "query_engine = index.as_query_engine(\n",
    "    response_nodes = \"tree_summarize\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build index\n",
    "# index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "#configure a retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "\n",
    "#configure a response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "\n",
    "#assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.query_engine import CustomQueryEngine\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from llama_index.response_synthesizers import BaseSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGQueryEngine(CustomQueryEngine):\n",
    "    retriever: BaseRetriever\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "\n",
    "    def custom_query(self, query_str: str):\n",
    "        nodes = self.retriever.retrieve(query_str)\n",
    "        response_obj = self.response_synthesizer.synthesize(query_str, nodes)\n",
    "        return response_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all query engines need to be learnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Response modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- refine: separate llm calls for each node to refine answer from each node\n",
    "    - ans_1 = node_1 + text_qa_template\n",
    "    - ans_2 = ans_1 + node_2 + refine_template\n",
    "    - .\n",
    "    - .\n",
    "    - ans_n = ans_n-1 + node_n + refine_template\n",
    "\n",
    "\n",
    "- compact: concatenate as many nodes as context window allows with the text_qa_template or refine_template, then work like refine\n",
    "- tree_summarize: summary_template + concatenated_node_chunks + T\n",
    "    - where T = next_layer(summary_template + concatenated_node_chunks + T)\n",
    "\n",
    "\n",
    "- simple_summarize: quick and lossy\n",
    "- no_text: dont send data to llm. just select data snd show source with response.source_nodes.\n",
    "- accumulate: run llm separately on text chunks and then concatinate the answers.\n",
    "- compact_accumulate: compact + accumulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming the output response of the app: \n",
    "<b>for reducing the perceived latency</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chosen llm needs to support streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high level api\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low level api\n",
    "\n",
    "synth = get_response_synthesizer(streaming=True)\n",
    "query_engine = RetrieverQueryEngine(response_synthesizer=synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_response = query_engine.query(\"\")\n",
    "for text in streaming_response.response_gen:\n",
    "    #do something with text as they arrive\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively print the final text\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Engine\n",
    "\n",
    "Keeps track of conversations unlike a query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine()\n",
    "\n",
    "# response = chat_engine.chat(\"ask something to it\")\n",
    "\n",
    "# streaming_response = chat_engine.stream_chat(\"tell something\")\n",
    "# for token in streaming_response.resopnse_gen:\n",
    "#     print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.reset() # to reset the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_engine.chat_repl() #to enter interactive chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high level api\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chat_mode argument in the above function:**\n",
    "- best: react data agent or openai data agent\n",
    "- condense_question: user chat history = query for index\n",
    "- context: retrieve nodes for every user msg. Retrieved text is inserted into system prompt\n",
    "- condense_plus_context: condense + context\n",
    "- simple: no query engine involved.\n",
    "- react: same as best but a react data agent\n",
    "- openai: same as best but a openai data agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#low level composition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
